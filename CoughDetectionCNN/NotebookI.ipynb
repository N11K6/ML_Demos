{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Assembling a Dataset & Extracting the Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Data\n",
    "\n",
    "The first thing we need in order to create a model for cough detection and monitoring, is the data. \n",
    "\n",
    "Two types of audio were sought out in order to form our dataset: \n",
    "   1. The first is obviously samples of people coughing. No further label is necessary at this point, however it is useful to include examples of all sexes and ages. \n",
    "   2. The second is any type of sound that is not a cough. As the intended implementation of this model is as an overnight monitoring module, sounds that have a high likelyhood of being heard in such a scenario were sought out. These include: random talking, snoring, footsteps, doors & drawers being opened and shut, street noises as recorded from indoors.\n",
    "\n",
    "An existing snoring dataset was very helpful in assembing our own. A number of audio samples was used from:\n",
    "T. H. Khan, \"A deep learning model for snoring detection and vibration notification using a smart wearable gadget,\" Electronics, vol. 8, no. 9, article. 987, ISSN 2079-9292, 2019.\n",
    "\n",
    "All other audio has been obtained from open access sources, extracted from youtube videos, or recorded by the author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries to be used\n",
    "\n",
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "import os\n",
    "from os import walk\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from pydub.silence import split_on_silence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The various raw audio files have been stored in two directories named \"various cough sounds\" and \"various non-cough sounds\". These files are uneven in length and contain multiple instances of the acoustic events in question. \n",
    "\n",
    "A first step we have to take is to split these raw audio files into segments each containing a single event (cough, or a non-cough instance). We chose to save the segments as wav files since they are much more convenient to work with than mp3s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolate_events(loadpath, savepath, min_silence_len, silence_thresh):\n",
    "    '''Function to split raw audio into chunks corresponding to isolated events\n",
    "    takes a specified loading path, a saving path, the minimum silence time length \n",
    "    in ms, and the threshold for silence in dB.'''\n",
    "    \n",
    "    sound_file = AudioSegment.from_mp3(loadpath)\n",
    "    \n",
    "    # split audio\n",
    "    audio_chunks = split_on_silence(sound_file, \n",
    "                                    # must be silent for at least half a second\n",
    "                                    min_silence_len=min_silence_len,\n",
    "                                    # consider it silent if quieter than \n",
    "                                    silence_thresh=silence_thresh\n",
    "                                   )\n",
    "    # store the \n",
    "    for i, chunk in enumerate(audio_chunks):\n",
    "\n",
    "        out_file = savepath+i+\".wav\"\n",
    "        print(\"exporting\", out_file)\n",
    "        chunk.export(out_file, format=\"wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the directories exist to store the segmented audio:\n",
    "if not os.path.exists(\"cough_segments\"):\n",
    "    os.makedirs(\"cough_segments\")\n",
    "if not os.path.exists(\"nocough_segments\"):\n",
    "    os.makedirs(\"nocough_segments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we can use the above function to extract audio events from our raw audio files. We will not automate this process further, as each raw audio file requires specific settings for the minimum silence length, and the silence threshold, that are better fine tuned manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cough audio events - or change the paths for the non-coughing events:\n",
    "LOADPATH = \"./various cough sounds/coughing_long (0).mp3\"\n",
    "SAVEPATH = \"./cough_segments/cough\"\n",
    "isolate_events(LOADPATH, SAVEPATH, 500, -35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Features\n",
    "\n",
    "So now we have a dataset full of short audio files, each corresponding to a single event (cough or no-cough). \n",
    "\n",
    "What is next, is to generate features we can train a model on out of this audio data. A good choice when it comes to human sounds is to use the Mel Frequency Ceptral Coefficients. We will not go into detail on what these are, but librosa has a convenient method to extract and plot these features. The resulting plots will be saved as png images.\n",
    "\n",
    "An example of an MFCC plot is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50538"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 224.64x224.64 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check image output\n",
    "\n",
    "filepath = './cough_segments/cough405.wav'\n",
    "clip, sample_rate = librosa.load(filepath, sr=None)\n",
    "\n",
    "# Stretch clip to 1s if too short, or cut it to 2s if too long:\n",
    "if len(clip) <= sample_rate:\n",
    "    clip = np.append(clip, np.zeros(sample_rate-len(clip)))\n",
    "elif len(clip) >= 2 * sample_rate:\n",
    "    clip = clip[:sample_rate]\n",
    "\n",
    "# Set up figure:\n",
    "fig = plt.figure(figsize=[3.12,3.12])  #[3.12,3.12])\n",
    "ax = fig.add_subplot(111)\n",
    "ax.axes.get_xaxis().set_visible(False)\n",
    "ax.axes.get_yaxis().set_visible(False)\n",
    "ax.set_frame_on(False)\n",
    "\n",
    "# Calculate and plot MFCCs\n",
    "S = librosa.feature.mfcc(y=clip, sr=sample_rate)\n",
    "librosa.display.specshow(librosa.power_to_db(S, ref=np.max))\n",
    "len(clip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is an example of the MFCC for a cough event. Let's see what it looks like for something that is not a cough:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44100"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 224.64x224.64 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filepath = './nocough_segments/0_200.wav'\n",
    "clip, sample_rate = librosa.load(filepath, sr=None)\n",
    "if len(clip) >= sample_rate:\n",
    "    clip = clip[:sample_rate]\n",
    "else:\n",
    "    clip = np.append(clip, np.zeros(sample_rate-len(clip)))\n",
    "    \n",
    "fig = plt.figure(figsize=[3.12,3.12])  #[3.12,3.12])\n",
    "ax = fig.add_subplot(111)\n",
    "ax.axes.get_xaxis().set_visible(False)\n",
    "ax.axes.get_yaxis().set_visible(False)\n",
    "ax.set_frame_on(False)\n",
    "\n",
    "S = librosa.feature.mfcc(y=clip, sr=sample_rate)\n",
    "librosa.display.specshow(librosa.power_to_db(S, ref=np.max))\n",
    "len(clip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To us humans, and from just two examples, it might seem impossible to differentiate between the two classes. However, given enough input, and with well structured model, we can have a tool to detect patterns and perform the classification task. \n",
    "\n",
    "In short, what we are doing is representing sound with pictures. A great deal of the properties that define each audio sample are contained in these MFCC images. Therefore, we can rely on a model that would normally be used to classify images to perform the classification of audio events.\n",
    "\n",
    "The next step is to create an image dataset out of the audio samples we have extracted, using the MFCC transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the directories exist to store the MFCC images:\n",
    "if not os.path.exists(\"cough_mfccs_test\"):\n",
    "    os.makedirs(\"cough_mfccs_test\")\n",
    "if not os.path.exists(\"nocough_mfccs_test\"):\n",
    "    os.makedirs(\"nocough_mfccs_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The directory paths in following code can be modified to select according to the class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_files = []\n",
    "\n",
    "# go through the audio files in the desired directory:\n",
    "for (_,_,filenames) in walk('cough_segments_test'):\n",
    "    audio_files.extend(filenames)\n",
    "    break\n",
    "    \n",
    "for i, audio_file in enumerate(audio_files):\n",
    "    \n",
    "    # read audio samples\n",
    "    input_data, sample_rate = librosa.load(\"cough_segments_test/\"+audio_file, sr=None)\n",
    "    \n",
    "    # ensure sample is of a reasonable length between 1 and 2 s:\n",
    "    if len(input_data) >= sample_rate:\n",
    "        input_data = input_data[:sample_rate]\n",
    "    else:\n",
    "        input_data = np.append(input_data, np.zeros(sample_rate-len(input_data)))\n",
    "    \n",
    "    # define figure parameters\n",
    "    fig = plt.figure(figsize=[3.12,3.12])\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.axes.get_xaxis().set_visible(False)\n",
    "    ax.axes.get_yaxis().set_visible(False)\n",
    "    ax.set_frame_on(False)\n",
    "\n",
    "    # calculate MFCCs and generate plot\n",
    "    S = librosa.feature.mfcc(y=input_data, sr=sample_rate)\n",
    "    librosa.display.specshow(librosa.power_to_db(S, ref=np.max))\n",
    "\n",
    "    # store the MFCC image\n",
    "    plt.savefig(\"cough_mfccs_test/cough_test\" + str(i) + '.png')\n",
    "\n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So by now we should have an image dataset of two classes, corresponding to cough and no-cough audio events. Using this data as input to our machine learning model, we should be able to train it so that it can detect coughing in an input audio stream.\n",
    "\n",
    "This is the end of Part I, see Part II in the second Notebook, where we compile and train our model using the features we just created."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
